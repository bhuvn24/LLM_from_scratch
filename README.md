# LLM-from-scratch

Build, train, and evaluate a minimalist GPT-style Large Language Model from first principles. This repo walks through tokenization (byte-level BPE), Transformer blocks (multi-head causal self-attention + MLP), training loops in PyTorch, and text sampling. It prioritizes clarity over cleverness, so you can learn, hack, and extend easily.

<p align="center"> <img src="https://img.shields.io/badge/Python-3.10%2B-blue" /> <img src="https://img.shields.io/badge/PyTorch-2.x-red" /> <img src="https://img.shields.io/badge/License-MIT-green" /> <img src="https://img.shields.io/badge/PRs-welcome-brightgreen" /> </p>

