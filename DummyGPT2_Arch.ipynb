{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            # nn.ReLU(),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT ARCHITECTURE : ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n",
    "print(batch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: CREATE AN INSTANCE OF DUMMYGPTMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4223, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT ARCHITECTURE : GENERATING TEXT FROM OUTPUT TOKENS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we put the model into .eval() mode, which disables random components like dropout, which are only used during training, and use the generate_text_simple function on the encoded input tensor:\n",
    "We disable dropout since we are not training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, based on the preceding output, the model generated gibberish, which is not at all coherent text. What happened? The reason why the model is unable to produce coherent text is that we haven't trained it yet. So far, we just implemented the GPT architecture and initialized a GPT model instance with initial random weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024) due to computational resource requirements \n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the text generation loss: cross-entropy and perplexity¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3626, 6100,  345])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.8849e-05, 1.5172e-05, 1.1687e-05,  ..., 2.2409e-05,\n",
      "          6.9776e-06, 1.8776e-05],\n",
      "         [9.1569e-06, 1.0062e-05, 7.8786e-06,  ..., 2.9090e-05,\n",
      "          6.0103e-06, 1.3571e-05],\n",
      "         [2.9877e-05, 8.8507e-06, 1.5741e-05,  ..., 3.5456e-05,\n",
      "          1.4094e-05, 1.3526e-05]],\n",
      "\n",
      "        [[1.2561e-05, 2.0537e-05, 1.4332e-05,  ..., 1.0389e-05,\n",
      "          3.4784e-05, 1.4239e-05],\n",
      "         [7.2731e-06, 1.7864e-05, 1.0565e-05,  ..., 2.1207e-05,\n",
      "          1.1390e-05, 1.5559e-05],\n",
      "         [2.9496e-05, 3.3605e-05, 4.1029e-05,  ..., 6.5249e-06,\n",
      "          5.8203e-05, 1.3698e-05]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas=torch.softmax(logits,dim=-1)\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "out = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Real:\\n\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to maximize all these values, bringing them close to a probability of 1. In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to make this average log probability as large as possible by optimizing the model weights. Due to the log, the largest possible value is 0, and we are currently far away from 0. In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the negative average log-probability value; in our case, instead of maximizing -10.7722 so that it approaches 0, in deep learning, we would minimize 10.7722 so that it approaches 0. The value negative of -10.7722, i.e., 10.7722, is also called cross-entropy loss in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already implements a cross_entropy function that carries out the previous steps Before we apply the cross_entropy function, let's check the shape of the logits and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "\n",
    "\n",
    "# For the cross_entropy function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:\n",
    "\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the training and validation set losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(text_data))\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_text_simple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m start_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvery effort moves you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mget_encoding(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text_simple\u001b[49m(\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     idx\u001b[38;5;241m=\u001b[39mtext_to_token_ids(start_context, tokenizer),\n\u001b[1;32m     18\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     19\u001b[0m     context_size\u001b[38;5;241m=\u001b[39mGPT_CONFIG_124M[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, token_ids_to_text(token_ids, tokenizer))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_text_simple' is not defined"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x119394c20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "9\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 9.046892589992947\n",
      "Validation loss: 9.115191459655762\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "# else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.771, Val loss 10.014\n",
      "Ep 1 (Step 000005): Train loss 8.128, Val loss 8.376\n",
      "Every effort moves you.                                                 \n",
      "Ep 2 (Step 000010): Train loss 6.684, Val loss 7.034\n",
      "Ep 2 (Step 000015): Train loss 6.136, Val loss 6.571\n",
      "Every effort moves you.                                                 \n",
      "Ep 3 (Step 000020): Train loss 5.647, Val loss 6.442\n",
      "Ep 3 (Step 000025): Train loss 5.396, Val loss 6.395\n",
      "Every effort moves you.                                                 \n",
      "Ep 4 (Step 000030): Train loss 5.227, Val loss 6.336\n",
      "Ep 4 (Step 000035): Train loss 4.716, Val loss 6.229\n",
      "Every effort moves you, I had been, I had been. I had been. \" a. I had been, and my, I had been, and I had been his I had been. I had been, I had been--and, and I had\n",
      "Ep 5 (Step 000040): Train loss 4.417, Val loss 6.220\n",
      "Every effort moves you I had been.         \"I had been to my work, and my work, and I said, and I had been that he had been the first I had the donkey, and I had been the first\n",
      "Ep 6 (Step 000045): Train loss 4.027, Val loss 6.150\n",
      "Ep 6 (Step 000050): Train loss 3.684, Val loss 6.139\n",
      "Every effort moves you know to the picture and I felt. Gisburn--as such--as.            \"Oh, I had been the donkey--and I saw that, and my work.   \n",
      "Ep 7 (Step 000055): Train loss 2.749, Val loss 6.147\n",
      "Ep 7 (Step 000060): Train loss 2.324, Val loss 6.204\n",
      "Every effort moves you know you know, and I felt.     \"I turned back to my work, and went on groping and m, and threw back his pictures, I had always--and, the donkey.      \n",
      "Ep 8 (Step 000065): Train loss 1.909, Val loss 6.210\n",
      "Ep 8 (Step 000070): Train loss 1.912, Val loss 6.223\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact by his last word.        \"Oh, and I said--as Jack himself, my; and I saw that, and my dear, and in the\n",
      "Ep 9 (Step 000075): Train loss 1.476, Val loss 6.236\n",
      "Ep 9 (Step 000080): Train loss 1.207, Val loss 6.257\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little a flash that he was a year after Jack's resolve had been taken. It might be that he had married her--the his pictures by a little to have he was.\n",
      "Ep 10 (Step 000085): Train loss 0.909, Val loss 6.293\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  \"Oh, a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "Training completed in 6.28 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECODING STRATEGIES TO CONTROL RANDOMNESS¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECODING STRATEGY 1: TEMPERATURE SCALING¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, inside the generate_text_simple function, we always sampled the token with the highest probability as the next token using torch.argmax, also known as greedy decoding. To generate text with more variety, we can replace the argmax with a function that samples from a probability distribution (here, the probability scores the LLM generates for each vocabulary entry at each token generation step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas=torch.softmax(next_token_logits,dim=0)\n",
    "next_token_id=torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a probabilistic sampling process, we can now replace the argmax with the multinomial function in PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printed output is \"forward\" just like before. What happened? The multinomial function samples the next token proportional to its probability score. In other words, \"forward\" is still the most likely token and will be selected by multinomial most of the time but not all the time. To illustrate this, let's implement a function that repeats this sampling 1000 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([6.0907e-02, 1.6313e-03, 1.0019e-04, 5.7212e-01, 3.4190e-03, 1.3257e-04,\n",
       "         1.0120e-04, 3.5758e-01, 4.0122e-03]),\n",
       " tensor([1.8530e-10, 3.5189e-26, 2.6890e-38, 9.9099e-01, 5.7569e-23, 4.4220e-37,\n",
       "         2.9718e-38, 9.0133e-03, 2.8514e-22]),\n",
       " tensor([0.1546, 0.0750, 0.0429, 0.2421, 0.0869, 0.0454, 0.0430, 0.2203, 0.0898])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECODING STRATEGY 2: Top-k sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Temperature Scaling and Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Mrs. Gis surprise a a little his glory just a good behind by\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING AND SAVING MODEL WEIGHTS IN PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the optimizer state is also recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING PRETRAINED WEIGHTS FROM OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 2.15.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow>=2.15.0 tqdm>=4.66\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshshivhare/LLMs-from-scratch/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"tqdm version:\", tqdm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshshivhare/LLMs-from-scratch/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 86.2kiB/s]\n",
      "/Users/harshshivhare/LLMs-from-scratch/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:09<00:00, 107kiB/s] \n",
      "/Users/harshshivhare/LLMs-from-scratch/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 196kiB/s]\n",
      "/Users/harshshivhare/LLMs-from-scratch/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [1:08:30<00:00, 121kiB/s]  \n",
      "/Users/harshshivhare/LLMs-from-scratch/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 5.60MiB/s]\n",
      "/Users/harshshivhare/LLMs-from-scratch/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:01<00:00, 397kiB/s]  \n",
      "/Users/harshshivhare/LLMs-from-scratch/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:02<00:00, 219kiB/s]  \n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.08716474e-03  3.65293846e-02 -6.72961622e-02  1.64160519e-04\n",
      " -6.74439520e-02 -7.13511184e-02  5.03934145e-01  9.17233601e-02\n",
      " -4.93398793e-02  3.26218014e-03  4.57228050e-02 -6.86740363e-03\n",
      "  2.40390748e-02 -2.34807320e-02 -1.67240128e-02 -1.71441715e-02\n",
      " -8.37184954e-03 -3.15133706e-02 -6.86011091e-02 -2.37664729e-01\n",
      "  3.62373106e-02  1.63457785e-02 -9.85065103e-02 -2.12323642e-03\n",
      "  1.19824829e-02  2.89787594e-02 -1.28519580e-01 -1.59477200e-02\n",
      " -1.98860541e-02 -5.36682643e-02  6.94034481e-03 -7.68073741e-03\n",
      " -1.09037161e-01 -1.99773069e-02  9.60835721e-03  1.08907633e-01\n",
      "  4.50431108e+00 -2.63199210e-02 -3.55628207e-02  6.86336830e-02\n",
      " -2.67327446e-02  4.53805970e-03 -7.64942616e-02 -1.86267532e-02\n",
      "  2.77857296e-04  1.70487557e-02  1.54484473e-02 -2.00693402e-02\n",
      " -9.41062346e-02  1.29353702e-01 -2.86019389e-02  1.79713070e-02\n",
      " -1.22552529e-01 -6.00398742e-02 -2.71516349e-02  2.08676040e-01\n",
      " -7.33136535e-02 -1.88488979e-02  3.25500555e-02 -1.03817329e-01\n",
      "  3.34629565e-02 -5.78856654e-02  2.70340182e-02 -1.67447120e-01\n",
      " -1.04716480e+00 -1.65415742e-02 -5.10184504e-02 -6.86529465e-03\n",
      " -2.12486312e-01 -3.12094428e-02 -6.06670119e-02  8.76753707e-04\n",
      "  2.86593381e-02 -2.42795385e-02 -7.43337721e-03 -1.09843947e-02\n",
      " -9.65665281e-02 -4.02747065e-01  3.52086276e-02  4.55822842e-03\n",
      " -7.44722877e-03 -1.46752540e-02  1.04234060e-02 -7.64231011e-02\n",
      "  1.57221835e-02  2.25943122e-02  3.48472804e-01 -9.92369652e-01\n",
      " -1.45732667e-02  4.18468639e-02 -5.65486178e-02 -6.41948357e-02\n",
      " -7.88374469e-02 -8.60625580e-02 -1.18104629e-02  1.16165662e-02\n",
      " -4.38655959e-03  4.70451377e-02  1.48782572e-02  1.54241994e-01\n",
      " -1.96132176e-02 -1.07700340e-01  1.12701245e-01 -6.45225262e-03\n",
      "  4.04450037e-02 -2.77711451e-02 -1.73039921e-02  7.37615108e-01\n",
      " -2.59598009e-02 -2.73887869e-02 -3.10639516e-02  1.57343205e-02\n",
      " -5.22806160e-02  4.12522145e-02 -1.33163435e-02 -4.50487584e-02\n",
      " -1.81804970e-03  7.23203970e-03  1.36934211e-02 -3.94124538e-02\n",
      " -3.47244218e-02  5.10589406e-02 -2.05698311e-02 -2.16058791e-02\n",
      " -1.35003313e-01  5.45603558e-02 -3.66716832e-02  2.26304103e-02\n",
      " -1.09535679e-01 -1.36641264e-02  5.51584847e-02  2.88132224e-02\n",
      "  2.73075644e-02 -5.69424368e-02 -7.86535889e-02  2.35797185e-03\n",
      "  2.00067107e-02 -5.71159460e-02  6.47636712e-01  1.01848580e-01\n",
      " -6.16239235e-02 -3.76336612e-02 -3.98378730e-01  2.14295574e-02\n",
      " -1.86435971e-02  1.96925759e-01  2.26767473e-02 -6.59890622e-02\n",
      " -4.36324105e-02  9.95995849e-03  2.40319856e-02 -1.00954473e-01\n",
      " -4.65803482e-02  2.91368607e-02 -5.90293519e-02 -3.26121263e-02\n",
      " -1.97375030e-03  2.41916650e-03 -3.74471582e-02 -3.01082209e-02\n",
      " -6.37610033e-02  3.36315818e-02 -1.85299199e-02  2.21461318e-02\n",
      " -6.41063303e-02  7.48323500e-02 -9.56049338e-02 -3.62511635e-01\n",
      " -3.00681647e-02 -1.14445284e-01 -4.17052880e-02 -7.91808069e-02\n",
      "  4.70847212e-04  8.24927837e-02 -5.66072389e-02 -3.38067114e-02\n",
      " -2.86269546e-01 -4.72303219e-02 -1.13803893e-01 -4.08402644e-02\n",
      "  5.71122905e-03 -2.54525281e-02  6.46117181e-02  3.44920047e-02\n",
      "  2.05656718e-02 -4.52667698e-02 -3.46782543e-02 -7.26115098e-03\n",
      " -1.89079233e-02  5.78902178e-02 -4.21664789e-02 -6.75565824e-02\n",
      "  5.65225594e-02 -2.84673963e-02  3.54542695e-02  1.52897201e-02\n",
      "  1.27683934e-02 -2.85420753e-02  2.31535453e-02 -2.09373772e-01\n",
      "  4.46508676e-02 -9.43315588e-03 -1.60416588e-02  1.32853054e-02\n",
      " -1.30786551e-02  5.23816503e-04  1.30414544e-02 -2.70975083e-02\n",
      "  4.45489064e-02 -8.77902657e-03 -1.46466028e-02  9.17641595e-02\n",
      " -3.12796757e-02  2.31977299e-01  4.41231057e-02  1.43043287e-02\n",
      " -5.72979972e-02  7.86180887e-03 -1.02373504e-03  2.84854975e-02\n",
      "  1.32676680e-02 -9.54552367e-03 -2.31092941e-04 -2.03693341e-02\n",
      "  8.33938718e-02 -7.11351782e-02 -6.48065731e-02 -6.11805031e-03\n",
      "  4.67911810e-02 -9.20884591e-03  3.48836780e-02 -4.50286306e-02\n",
      "  2.78786391e-01 -3.78183313e-02 -5.40339835e-02 -6.17898814e-02\n",
      " -3.87775712e-02  4.93486738e-03  2.43781991e-02  8.90252180e-03\n",
      " -3.50524625e-03  1.65808886e-01  5.86637035e-02 -3.12985433e-03\n",
      "  2.79885661e-02  2.58715264e-02 -1.11185312e-02  7.60973915e-02\n",
      " -2.22281944e-02 -1.96890365e-02 -1.37841865e-01 -6.64365590e-02\n",
      " -5.29131293e-03  1.70096580e-03 -2.64494848e-02  1.17172316e-01\n",
      " -1.15228012e-01  7.00972080e-02 -8.22834224e-02  6.10999763e-02\n",
      " -1.21604139e-02  9.09516066e-02  7.55620981e-03  3.88195589e-02\n",
      "  1.94660556e-02 -6.44877478e-02 -9.73987043e-01  1.06707960e-03\n",
      " -7.21861888e-03  8.74927118e-02 -1.15468912e-01  4.37375754e-01\n",
      "  5.46969986e-03 -1.57152023e-02  2.89246906e-02 -3.34697738e-02\n",
      " -4.21249233e-02  1.28952079e-02 -2.97768563e-02 -1.70153920e-02\n",
      " -2.27351002e-02  1.70611255e-02  4.95454781e-02  1.11035757e-01\n",
      " -5.83135430e-03  5.74073195e-02 -2.28053350e-02  2.09412258e-02\n",
      " -3.50160182e-01 -2.16342770e-02 -5.81450686e-02 -2.18734592e-02\n",
      " -1.36683295e-02  1.28618302e-02  3.67184468e-02  5.19019552e-03\n",
      "  1.09068519e-02 -7.82321487e-03 -4.59075831e-02 -1.73661225e-02\n",
      " -2.84003876e-02  4.18591648e-02 -1.03669748e-01  2.98754182e-02\n",
      " -4.32000756e-02 -2.74724141e-02  7.61727849e-03 -7.89048988e-03\n",
      " -4.42346692e-01 -2.45629009e-02 -1.76335108e-02  1.24419190e-01\n",
      "  3.79812978e-02 -7.63773220e-03 -4.19176531e+00 -5.05156219e-02\n",
      "  2.93693896e-02 -9.71736666e-03  7.48562597e-05  3.83218117e-02\n",
      " -2.17752025e-01 -1.39279827e-03 -4.34128046e-02 -4.78117056e-02\n",
      "  1.06232045e-02 -5.46386987e-02 -5.31872392e-01 -2.25281809e-02\n",
      "  1.07347462e-02  7.71407643e-03 -1.18210004e-03 -7.19227418e-02\n",
      " -6.68982491e-02 -9.45700109e-02  1.82749536e-02 -5.68016386e-03\n",
      "  3.39248888e-02 -6.80351537e-03 -2.11909004e-02 -3.36676091e-02\n",
      " -4.57466543e-02 -1.71643738e-02 -6.41259626e-02 -5.93215674e-02\n",
      "  6.63078483e-03 -5.73917739e-02 -2.92235091e-02 -1.02373123e-01\n",
      " -2.10061017e-02  3.76819260e-02 -8.32376722e-03  1.86293066e-01\n",
      " -3.02889291e-02 -8.61082599e-02 -3.39810699e-02 -1.63804623e-03\n",
      "  9.56450458e-05  1.83909796e-02  6.61542034e-03 -2.57867929e-02\n",
      " -5.49653843e-02  3.35317068e-02  7.64138177e-02 -3.54181342e-02\n",
      "  1.57574439e-04 -4.92151350e-01 -1.79349035e-02 -4.24741097e-02\n",
      " -3.66543941e-02  3.71866580e-03  8.37213323e-02 -7.39048719e-02\n",
      "  1.33489156e-02 -8.12511027e-01 -2.29635882e+00 -2.36717984e-02\n",
      " -8.13344792e-02  1.50864288e-01 -7.69050941e-02 -3.10524683e-02\n",
      "  2.14246698e-02 -4.83890735e-02 -1.41519252e-02 -4.39345315e-02\n",
      " -3.75095680e-02 -1.15410060e-01  6.28832355e-02 -1.76720843e-02\n",
      "  1.09622553e-02 -2.61482865e-01  5.88595793e-02  4.11713496e-02\n",
      " -5.27693182e-02 -5.32636344e-01 -7.30699822e-02 -3.02720144e-02\n",
      " -3.35028954e-02 -2.33484805e-02  1.68391466e-02  1.38347782e-02\n",
      " -3.15353759e-02 -1.29300281e-01  8.20337236e-02 -1.76686160e-02\n",
      " -3.70116755e-02 -5.35593703e-02 -8.07929933e-02 -4.50296849e-02\n",
      "  5.83262444e-01 -2.80185509e-02 -2.28541084e-02 -2.94910222e-02\n",
      " -1.26462365e-02 -1.90845113e-02  7.27776289e-02 -2.99875624e-02\n",
      " -4.30349773e-03 -2.31713429e-02 -2.91151367e-02  2.83182506e-03\n",
      " -1.90562889e-01 -3.66036184e-02  1.49339819e-02 -7.27724805e-02\n",
      "  1.73824802e-02  1.31842634e-02  3.31939273e-02  1.20899454e-02\n",
      " -6.78259432e-02 -4.20462340e-02  4.57649612e+00  3.80943827e-02\n",
      " -5.84753277e-03 -3.30598466e-02 -2.44196644e-03  7.05721304e-02\n",
      " -8.51837173e-03 -4.71400023e-02 -3.54915038e-02  1.39056429e-01\n",
      "  9.68060642e-03  3.77591630e-03  1.52141786e+00  1.57720726e-02\n",
      " -2.01463164e-03  1.98064416e-04 -2.59160306e-02  7.45529830e-01\n",
      "  2.25554965e-02  3.91503461e-02 -4.06123102e-02 -6.64877519e-02\n",
      " -1.46940826e-02 -1.88139483e-01 -2.56802626e-02  2.75394171e-01\n",
      " -8.66981745e-02 -5.40682413e-02 -3.77960838e-02  1.74960971e-01\n",
      "  1.00158982e-01  1.82876736e-02  1.90151744e-02 -4.92472537e-02\n",
      " -1.13439135e-01  3.24170664e-03  1.50688691e-02 -2.39907298e-02\n",
      "  1.11687267e-02  5.52010946e-02  6.88129589e-02  1.54367080e-02\n",
      "  1.44566922e-02  3.63817438e-02  3.30061018e-02  1.66012859e-03\n",
      "  3.58405709e-02  2.30494030e-02 -3.78327258e-02 -8.89169157e-01\n",
      " -8.56258094e-01  7.65006363e-01 -1.66502837e-02  5.34249656e-03\n",
      " -2.09745876e-02 -2.09852960e-02 -2.54212581e-02  1.16072060e-03\n",
      " -2.56687868e-02  1.16133735e-04 -2.64434293e-02 -1.28211202e-02\n",
      " -1.10604595e-02 -6.45856336e-02 -1.67003309e-03  6.04913272e-02\n",
      "  7.36832380e+00 -6.02299497e-02 -2.91371737e-02 -1.25804571e-02\n",
      " -5.30599356e-02 -1.85821671e-02 -1.78866498e-02 -2.96165675e-01\n",
      "  8.00850391e-02  5.40530123e-02 -2.81387884e-02 -8.78574979e-03\n",
      "  1.09404707e-02 -1.99578963e-02 -3.99825945e-02 -2.50019968e-01\n",
      "  9.08077285e-02 -2.37536663e-03  4.64344630e-03 -6.04528897e-02\n",
      "  6.99793268e-03 -3.82795855e-02 -5.84160350e-02 -1.83243211e-02\n",
      " -2.63099913e-02  8.23536813e-02  8.08781944e-03 -6.16159998e-02\n",
      " -4.63683382e-02 -1.12056546e-01  2.27390423e-01 -1.91568092e-01\n",
      " -2.89059915e-02  3.94524541e-03  1.52852330e-02 -8.96849576e-03\n",
      " -4.17652465e-02  2.63159387e-02 -4.13656868e-02 -6.12082472e-03\n",
      " -1.07259089e-02 -5.37239276e-02 -1.10639390e-02  5.46083637e-02\n",
      "  1.20256595e-01 -5.97908273e-02 -1.14393644e-02 -3.02061826e-01\n",
      " -6.60821516e-03  7.50206411e-02  3.34696583e-02  3.19551677e-02\n",
      " -8.96558631e-03 -2.41305437e-02  2.00992599e-02  1.79016620e-01\n",
      " -4.71279696e-02  3.53363827e-02 -5.99645413e-02 -1.07290419e-02\n",
      "  2.55345963e-02 -3.25982496e-02 -1.15713170e-02  3.92786972e-03\n",
      " -4.60971184e-02 -1.01251360e-02 -2.54677068e-02 -5.31320758e-02\n",
      "  1.04298815e-03 -5.78965321e-02  1.15620784e-01  1.66952014e-02\n",
      " -9.38584190e-03 -1.09665282e-01  1.13851644e-01 -1.25865871e-02\n",
      " -2.98552942e-02  6.65151887e-03  1.12767250e-03 -8.84453654e-02\n",
      "  3.56004350e-02 -4.76986393e-02 -6.59885025e-03 -2.53764316e-02\n",
      " -5.91694154e-02 -2.61245519e-02  3.56641524e-02  4.23303694e-02\n",
      " -2.83649862e-02 -1.14223681e-01 -1.27170356e-02 -5.48909865e-02\n",
      "  2.66137421e-02 -2.05100384e-02 -4.04552221e-02  2.58468129e-02\n",
      "  1.89122278e-02  3.98732815e-03  1.24724917e-02 -1.57784615e-02\n",
      " -4.39446196e-02 -3.98074389e-02  4.93680947e-02 -3.48111577e-02\n",
      " -3.67384851e-02  6.04443215e-02 -3.19826417e-02  2.10023075e-02\n",
      "  5.97873889e-02 -5.94177246e-02 -5.65183647e-02 -1.95623394e-02\n",
      " -1.01916380e-01 -7.32661784e-02  6.02526311e-03 -1.27025985e-03\n",
      " -1.76584590e-02 -1.70460073e-04 -2.20485535e-02 -4.48220596e-02\n",
      " -3.03197466e-02  6.13330817e-03 -7.39828646e-02 -3.90712023e-02\n",
      "  7.11266771e-02 -1.93825141e-02  8.13722014e-02 -3.76556404e-02\n",
      " -3.92407849e-02  1.13070533e-02  1.92395076e-02 -5.21337271e-01\n",
      " -5.71017750e-02  1.28721772e-02  2.64368802e-02 -2.46955734e-02\n",
      " -9.57516197e-04 -2.95170979e-03 -4.00305912e-02 -2.64029145e-01\n",
      " -1.84592158e-02 -4.37720865e-02 -6.42635077e-02  6.21688552e-04\n",
      " -4.29761380e-01 -5.21100424e-02 -2.76717655e-02 -1.32346153e-01\n",
      " -2.08241269e-02 -2.66745687e-02 -2.97277011e-02  4.17347662e-02\n",
      "  1.00565506e-02  5.22596315e-02 -7.44589642e-02 -2.18557473e-02\n",
      " -1.42166968e-02 -6.52396679e-02 -2.22352277e-02  1.69406831e-01\n",
      " -9.01447516e-03  2.41598208e-02 -2.97872201e-02  1.44478559e-01\n",
      "  1.95567477e-02 -5.58326999e-03 -5.05079888e-02  3.87011357e-02\n",
      " -2.96679605e-02  3.16897295e-02 -6.87445048e-03 -1.92143805e-02\n",
      "  2.69025415e-01 -7.60187069e-03 -1.56251132e-01 -2.65167132e-02\n",
      " -4.49076109e-03 -3.77961360e-02 -2.03038752e-01 -4.03431579e-02\n",
      " -2.97578759e-02  3.26392017e-02  6.78672194e-02  2.91899443e-02\n",
      " -8.35625529e-02 -7.64480531e-02 -7.06719756e-02  9.85725876e-03\n",
      "  3.59054729e-02  4.41314541e-02 -5.11193555e-03 -1.03020733e-02\n",
      "  1.41771482e-02  1.27768638e-02  4.40260209e-02  2.68844478e-02\n",
      "  5.46628349e-02 -5.95498867e-02  2.13864609e-03 -6.34008646e-02\n",
      " -4.61590812e-02  1.77550223e-02  9.30165127e-03 -1.39742559e-02\n",
      " -2.59907711e-02  6.99018463e-02 -2.04474572e-02 -8.09957564e-01\n",
      " -7.39924842e-03 -6.90279603e-02 -2.39470258e-01 -1.01591632e-01\n",
      " -3.58320251e-02 -1.92358851e-01 -4.73936349e-02 -1.67166686e-03\n",
      "  5.39828390e-02  1.05435411e-02  1.43036181e-02 -4.85613570e-02\n",
      " -4.24761958e-02 -8.77168321e-04  1.41238064e-01  9.46865184e-04\n",
      " -4.81572710e-02 -1.10126831e-01 -4.40879054e-02 -4.07132469e-02\n",
      " -5.97845986e-02 -1.61114130e-02  8.90779421e-02 -4.97539490e-02\n",
      " -6.37685210e-02  1.46076325e-02 -4.46997657e-02  1.03385877e-02\n",
      "  3.83437127e-02  5.58212362e-02 -4.39544134e-02  4.06824015e-02\n",
      "  1.24285175e-02  1.12647365e-03 -9.79550630e-02 -3.31693292e-02\n",
      " -5.41585982e-02 -6.51617050e-02 -6.45537227e-02  3.43673564e-02\n",
      " -5.53162992e-02  1.62390456e-01 -9.13274102e-03  2.19418630e-02\n",
      "  1.77566335e-03 -7.42564797e-02  2.28889417e-02 -9.64138880e-02\n",
      " -2.94263493e-02  2.18627490e-02 -2.45231111e-02  5.90597428e-02\n",
      "  2.93598205e-01  6.24936670e-02  3.84866929e-04  9.15394872e-02\n",
      "  3.11307814e-02  3.30191152e-03  4.57600713e-01 -5.75991087e-02\n",
      "  3.41239758e-02 -1.00945365e-02 -2.55375616e-02  3.44500057e-02]\n",
      "norm scale weight tensor dimensions: (768,)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"b\"])\n",
    "print(\"norm scale weight tensor dimensions:\", params[\"b\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 1024,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': True}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the GPTModel instance is initialized with random weights for pretraining. The last step to using OpenAI's model weights is to override these random weights with the weights we loaded into the params dictionary. For this, we will first define a small assign utility function that checks whether two tensors or arrays (left and right) have the same dimensions or shape and returns the right tensor as trainable PyTorch parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a load_weights_into_gpt function that loads the weights from the params dictionary into a GPTModel instance gpt:\n",
    "\n",
    "Step 1: Setting the model's positional and token embedding weights to those specified in params. Step 2: Iterate over each transformer block in the model. Step 3: The np.split function is used to divide the attention and bias weights into three equal parts for the query, key, and value components. Step 4: The original GPT-2 model by OpenAI reused the token embedding weights in the output layer to reduce the total number of parameters, which is a concept known as weight tying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    # weight tying as we use embedding layer weights only below\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Who are you planning to write a new chapter in your own voice? The more you write about it, the greater the exposure you get…\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Who are you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
